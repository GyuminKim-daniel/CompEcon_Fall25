{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521dac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gender-guesser\n",
    "\n",
    "# Full pipeline: load CSVs -> compute c_user (HHA-style) -> build pairs -> mixed-logit\n",
    "# Requirements: pandas, numpy, scipy, sklearn, statsmodels\n",
    "import time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from scipy import sparse, optimize, stats\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "import gender_guesser.detector as gd\n",
    "\n",
    "random.seed(1); np.random.seed(1)\n",
    "\n",
    "# -------------------------\n",
    "# 0. Load data (adjust filenames if necessary)\n",
    "# -------------------------\n",
    "users = pd.read_csv('Users.csv')\n",
    "orgs  = pd.read_csv('UserOrganizations.csv')\n",
    "ach   = pd.read_csv('UserAchievements.csv')\n",
    "teams = pd.read_csv('Teams.csv')\n",
    "tm    = pd.read_csv('TeamMemberships.csv')\n",
    "\n",
    "# normalize id columns\n",
    "if 'Id' in users.columns and 'UserId' not in users.columns:\n",
    "    users = users.rename(columns={'Id':'UserId'})\n",
    "if 'Id' in teams.columns and 'TeamId' not in teams.columns:\n",
    "    teams = teams.rename(columns={'Id':'TeamId'})\n",
    "\n",
    "# Keep competition achievements only\n",
    "if 'AchievementType' in ach.columns:\n",
    "    ach = ach[ach['AchievementType'] == 'Competitions'].copy()\n",
    "\n",
    "# Merge users -> organizations -> achievements\n",
    "users_org = users.merge(orgs, how='left', on='UserId')\n",
    "users_org_ach = users_org.merge(ach, how='left', on='UserId')\n",
    "\n",
    "# Merge team memberships -> teams -> user info to create 'merged'\n",
    "team_member = tm.merge(teams, how='left', on='TeamId')\n",
    "merged = team_member.merge(users_org_ach, how='left', on='UserId')\n",
    "\n",
    "# Ensure string IDs\n",
    "for col in ['UserId','TeamId','CompetitionId']:\n",
    "    if col in merged.columns:\n",
    "        merged[col] = merged[col].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8c36ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_comp: rows = 8615864 unique users = 3119546\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1. Build users_comp (one row per user-competition) and compute Solo indicator\n",
    "# -------------------------\n",
    "# team sizes per (TeamId, CompetitionId)\n",
    "mask_team = merged['TeamId'].notnull() & (merged['TeamId'] != 'nan')\n",
    "team_sizes = (merged[mask_team]\n",
    "              .groupby(['TeamId','CompetitionId'], dropna=False)\n",
    "              .size()\n",
    "              .reset_index(name='team_size'))\n",
    "\n",
    "merged = merged.merge(team_sizes, on=['TeamId','CompetitionId'], how='left')\n",
    "merged['team_size'] = merged['team_size'].fillna(1).astype(int)  # default 1 for solo/missing team\n",
    "\n",
    "users_comp = (merged[['CompetitionId','UserId','TeamId','team_size']]\n",
    "              .drop_duplicates()\n",
    "              .reset_index(drop=True))\n",
    "users_comp['Solo'] = ((users_comp['team_size'] == 1) | (users_comp['TeamId'].isna()) | (users_comp['TeamId']=='nan')).astype(int)\n",
    "\n",
    "# compute n_competitions per user\n",
    "ncomp = (merged[['UserId','CompetitionId']].drop_duplicates()\n",
    "         .groupby('UserId')['CompetitionId'].nunique().reset_index().rename(columns={'CompetitionId':'n_competitions'}))\n",
    "ncomp['UserId'] = ncomp['UserId'].astype(str)\n",
    "users_comp = users_comp.merge(ncomp, on='UserId', how='left')\n",
    "users_comp['n_competitions'] = users_comp['n_competitions'].fillna(0).astype(int)\n",
    "\n",
    "# pull HighestRanking into users_comp if available from users_org_ach\n",
    "if 'HighestRanking' in users_org_ach.columns:\n",
    "    tmp = users_org_ach[['UserId','HighestRanking']].drop_duplicates('UserId')\n",
    "    tmp['UserId'] = tmp['UserId'].astype(str)\n",
    "    users_comp = users_comp.merge(tmp, on='UserId', how='left')\n",
    "else:\n",
    "    users_comp['HighestRanking'] = np.nan\n",
    "\n",
    "print(\"users_comp: rows =\", len(users_comp), \"unique users =\", users_comp['UserId'].nunique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d2c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gender-guesser in c:\\users\\gyumin kim\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "         DisplayName first_name   Gender\n",
      "0    Kaggle Team Bot     Kaggle  unknown\n",
      "1  Anthony Goldbloom    Anthony     male\n",
      "2           Isabelle   Isabelle   female\n",
      "3      David Stephan      David     male\n",
      "4        Gabe Warren       Gabe     male\n",
      "Gender\n",
      "unknown    17147341\n",
      "male        6639358\n",
      "female      2901679\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Extract first name and infer gender using gender-guesser ---\n",
    "\n",
    "\n",
    "# Load users if not yet loaded\n",
    "try:\n",
    "    users\n",
    "except NameError:\n",
    "    users = pd.read_csv(\"Users.csv\")\n",
    "\n",
    "# Extract first name from DisplayName\n",
    "def extract_first_name(name):\n",
    "    if pd.isna(name): return ''\n",
    "    s = str(name).strip()\n",
    "    s = re.sub(r'\\(.*?\\)', '', s)\n",
    "    if ',' in s:  # handle \"Last, First\"\n",
    "        parts = [p.strip() for p in s.split(',') if p.strip()]\n",
    "        if len(parts) > 1: return parts[1].split()[0].capitalize()\n",
    "    return s.split()[0].capitalize() if s else ''\n",
    "\n",
    "users['first_name'] = users['DisplayName'].fillna('').astype(str).apply(extract_first_name)\n",
    "\n",
    "# Predict gender\n",
    "detector = gd.Detector(case_sensitive=False)\n",
    "users['Gender'] = users['first_name'].apply(lambda x: (\n",
    "    'male' if detector.get_gender(x) in ['male','mostly_male'] else\n",
    "    'female' if detector.get_gender(x) in ['female','mostly_female'] else\n",
    "    'unknown'\n",
    "))\n",
    "users['is_male'] = (users['Gender'] == 'male').astype(int)\n",
    "\n",
    "# Quick check\n",
    "print(users[['DisplayName','first_name','Gender']].head())\n",
    "print(users['Gender'].value_counts(dropna=False))\n",
    "\n",
    "# Merge into users_org_ach if it exists\n",
    "if 'users_org_ach' in globals():\n",
    "    users['UserId'] = users['UserId'].astype(str)\n",
    "    users_org_ach['UserId'] = users_org_ach['UserId'].astype(str)\n",
    "    users_org_ach = users_org_ach.merge(users[['UserId','first_name','Gender','is_male']], on='UserId', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7f6611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating FE for users with >=2 obs: 1307717 users; obs: 6804035\n",
      "FE fit time (s): 68.05\n",
      "c_user mean: 0.604 sd: 0.3841\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2. HHA-style person fixed effects c_user\n",
    "#    (fit penalized logistic with ONLY user dummies; no intercept or pooled regressors)\n",
    "# -------------------------\n",
    "# Keep users with >=2 observations for FE estimation (practical)\n",
    "counts = users_comp['UserId'].value_counts()\n",
    "users_keep = counts[counts >= 2].index.astype(str).tolist()\n",
    "uc_fe = users_comp[users_comp['UserId'].isin(users_keep)].reset_index(drop=True)\n",
    "print(\"Estimating FE for users with >=2 obs:\", len(users_keep), \"users; obs:\", len(uc_fe))\n",
    "\n",
    "if len(uc_fe) == 0:\n",
    "    raise RuntimeError(\"No users with >=2 observations â€” cannot estimate person FE. Consider lowering threshold.\")\n",
    "\n",
    "# build sparse user-dummy matrix\n",
    "users_unique = uc_fe['UserId'].unique()\n",
    "n_obs = len(uc_fe); n_users = len(users_unique)\n",
    "rows = np.arange(n_obs)\n",
    "cols = uc_fe['UserId'].map({u:i for i,u in enumerate(users_unique)}).values\n",
    "X_user = sparse.csr_matrix((np.ones(n_obs),(rows,cols)), shape=(n_obs,n_users))\n",
    "y = uc_fe['Solo'].astype(int).values\n",
    "\n",
    "# fit penalized logistic (no intercept)\n",
    "clf = LogisticRegression(penalty='l2', C=0.5, solver='saga', max_iter=1000, tol=1e-3, fit_intercept=False)\n",
    "t0 = time.time()\n",
    "clf.fit(X_user, y)\n",
    "t1 = time.time()\n",
    "print(\"FE fit time (s):\", round(t1-t0,2))\n",
    "\n",
    "coef_user = clf.coef_.ravel()\n",
    "user_c = pd.DataFrame({'UserId': users_unique.astype(str), 'c_user': coef_user})\n",
    "mean_c = user_c['c_user'].mean()\n",
    "print(\"c_user mean:\", round(mean_c,4), \"sd:\", round(user_c['c_user'].std(),4))\n",
    "\n",
    "# assign mean to users not estimated\n",
    "all_users = users_comp['UserId'].unique().astype(str)\n",
    "missing = set(all_users) - set(users_unique)\n",
    "if missing:\n",
    "    user_c = pd.concat([user_c, pd.DataFrame({'UserId':list(missing),'c_user':mean_c})], ignore_index=True)\n",
    "\n",
    "# also include users in users_org_ach not in user_c for safety\n",
    "extra = set(users_org_ach['UserId'].astype(str).unique()) - set(user_c['UserId'].astype(str))\n",
    "if extra:\n",
    "    user_c = pd.concat([user_c, pd.DataFrame({'UserId':list(extra),'c_user':mean_c})], ignore_index=True)\n",
    "\n",
    "# merge into users_org_ach for lookup\n",
    "users_org_ach['UserId'] = users_org_ach['UserId'].astype(str)\n",
    "users_org_ach = users_org_ach.merge(user_c[['UserId','c_user']], on='UserId', how='left')\n",
    "users_org_ach['c_user'] = users_org_ach['c_user'].fillna(mean_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e4a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ensure un-interacted fields exist in users_org_ach ---\n",
    "users_org_ach['HighestRanking'] = pd.to_numeric(\n",
    "    users_org_ach['HighestRanking'] if 'HighestRanking' in users_org_ach.columns else np.nan,\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Safe handling for n_competitions (avoid .fillna() on int)\n",
    "if 'n_competitions' in users_org_ach.columns:\n",
    "    users_org_ach['n_competitions'] = pd.to_numeric(users_org_ach['n_competitions'], errors='coerce').fillna(0)\n",
    "else:\n",
    "    users_org_ach['n_competitions'] = 0\n",
    "\n",
    "# is_male exists from earlier gender inference\n",
    "if 'is_male' not in users_org_ach.columns:\n",
    "    users_org_ach['is_male'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf76b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives: 366726\n",
      "Negatives sampled: 2433121\n",
      "Total pairs: 2780004 Positives: 346883\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 3. Build pair dataset: positives (team pairs) + sampled negatives (K per positive)\n",
    "# -------------------------\n",
    "# Build positives\n",
    "pos_rows = []\n",
    "for team, g in merged.groupby('TeamId'):\n",
    "    mems = sorted(g['UserId'].astype(str).unique())\n",
    "    if len(mems) < 2: \n",
    "        continue\n",
    "    comp = str(g['CompetitionId'].iloc[0])\n",
    "    m = len(mems)\n",
    "    for i in range(m):\n",
    "        for j in range(i+1, m):\n",
    "            pos_rows.append((comp, mems[i], mems[j], m))\n",
    "pos_df = pd.DataFrame(pos_rows, columns=['CompetitionId','User_i','User_j','team_size'])\n",
    "print(\"Positives:\", len(pos_df))\n",
    "\n",
    "# participant lists\n",
    "comp_users = merged.groupby('CompetitionId')['UserId'].unique().to_dict()\n",
    "for c in list(comp_users.keys()):\n",
    "    comp_users[c] = [str(x) for x in comp_users[c]]\n",
    "\n",
    "# sample negatives: K per positive (or 200 if no positives)\n",
    "K = 5\n",
    "neg_list = []\n",
    "for comp, users in comp_users.items():\n",
    "    users = sorted(users)\n",
    "    n = len(users)\n",
    "    if n < 2: continue\n",
    "    pos_c = pos_df[pos_df['CompetitionId']==comp]\n",
    "    P = len(pos_c)\n",
    "    target = K*P if P>0 else min(200, n*(n-1)//2)\n",
    "    sampled = set(); tries = 0\n",
    "    while len(sampled) < target and tries < target*20:\n",
    "        a,b = random.sample(users,2)\n",
    "        if a == b:\n",
    "            tries += 1; continue\n",
    "        if a > b: a,b = b,a\n",
    "        key = (a,b)\n",
    "        if key in sampled: tries += 1; continue\n",
    "        if ((pos_c['User_i']==a)&(pos_c['User_j']==b)).any():\n",
    "            tries += 1; continue\n",
    "        sampled.add(key)\n",
    "        neg_list.append((comp,a,b))\n",
    "        tries += 1\n",
    "\n",
    "neg_df = pd.DataFrame(neg_list, columns=['CompetitionId','User_i','User_j'])\n",
    "neg_df['team_size'] = 0\n",
    "print(\"Negatives sampled:\", len(neg_df))\n",
    "\n",
    "# combine\n",
    "pos_df['label'] = 1\n",
    "neg_df['label'] = 0\n",
    "pairs = pd.concat([pos_df, neg_df], ignore_index=True)\n",
    "pairs = pairs.drop_duplicates(subset=['CompetitionId','User_i','User_j']).reset_index(drop=True)\n",
    "print(\"Total pairs:\", len(pairs), \"Positives:\", int(pairs['label'].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1d51f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs with covariates ready. Example:\n",
      "  CompetitionId    User_i    User_j  label  same_org  same_country  \\\n",
      "0         45040  14029285   5176723      1         0             0   \n",
      "1         46105   1465504   9847588      1         0             0   \n",
      "2         37794  10291473  14029206      1         0             0   \n",
      "3         46801   4735773   8671882      1         0             0   \n",
      "4          7372    599610    611059      1         0             0   \n",
      "\n",
      "   abs_rank_diff  avg_c_neg  \n",
      "0            0.0  -0.258157  \n",
      "1            0.0  -1.029243  \n",
      "2            0.0  -0.047643  \n",
      "3            0.0  -0.534327  \n",
      "4            0.0  -0.181955  \n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 4. Merge user-level info (c_user and un-interacted terms) into pairs\n",
    "# -------------------------\n",
    "lookup = users_org_ach[['UserId','c_user','HighestRanking','n_competitions','is_male']].drop_duplicates('UserId').copy()\n",
    "lookup['UserId'] = lookup['UserId'].astype(str)\n",
    "\n",
    "pairs['User_i'] = pairs['User_i'].astype(str); pairs['User_j'] = pairs['User_j'].astype(str)\n",
    "pairs = pairs.merge(lookup.rename(columns={'UserId':'User_i','c_user':'ci','HighestRanking':'i_HighestRanking','n_competitions':'i_n_competitions','is_male':'i_is_male'}), on='User_i', how='left')\n",
    "pairs = pairs.merge(lookup.rename(columns={'UserId':'User_j','c_user':'cj','HighestRanking':'j_HighestRanking','n_competitions':'j_n_competitions','is_male':'j_is_male'}), on='User_j', how='left')\n",
    "\n",
    "# fill missing\n",
    "pairs['ci'] = pairs['ci'].fillna(mean_c)\n",
    "pairs['cj'] = pairs['cj'].fillna(mean_c)\n",
    "pairs['i_HighestRanking'] = pd.to_numeric(pairs['i_HighestRanking'], errors='coerce').fillna(-1)\n",
    "pairs['j_HighestRanking'] = pd.to_numeric(pairs['j_HighestRanking'], errors='coerce').fillna(-1)\n",
    "pairs['i_n_competitions'] = pd.to_numeric(pairs['i_n_competitions'], errors='coerce').fillna(0)\n",
    "pairs['j_n_competitions'] = pd.to_numeric(pairs['j_n_competitions'], errors='coerce').fillna(0)\n",
    "pairs['i_is_male'] = pairs['i_is_male'].fillna(0).astype(int)\n",
    "pairs['j_is_male'] = pairs['j_is_male'].fillna(0).astype(int)\n",
    "\n",
    "# pair covariates\n",
    "pairs['same_org'] = (pairs['ci'].notnull() & (pairs['ci'].notnull()) & (pairs['ci']*0==0)).astype(int)  # placeholder -> we'll compute properly below\n",
    "# Proper same_org: compare Organization if exists\n",
    "if 'Organization' in users_org_ach.columns:\n",
    "    u_org = users_org_ach[['UserId','Organization']].drop_duplicates('UserId').copy()\n",
    "    u_org['UserId'] = u_org['UserId'].astype(str)\n",
    "    pairs = pairs.merge(u_org.rename(columns={'UserId':'User_i','Organization':'i_Organization'}), on='User_i', how='left')\n",
    "    pairs = pairs.merge(u_org.rename(columns={'UserId':'User_j','Organization':'j_Organization'}), on='User_j', how='left')\n",
    "    pairs['same_org'] = (pairs['i_Organization'] == pairs['j_Organization']).astype(int)\n",
    "else:\n",
    "    # if Organization not available, set same_org=0\n",
    "    pairs['same_org'] = 0\n",
    "\n",
    "# same_country if available\n",
    "if 'Country' in users_org_ach.columns:\n",
    "    u_ct = users_org_ach[['UserId','Country']].drop_duplicates('UserId').copy()\n",
    "    u_ct['UserId'] = u_ct['UserId'].astype(str)\n",
    "    pairs = pairs.merge(u_ct.rename(columns={'UserId':'User_i','Country':'i_Country'}), on='User_i', how='left')\n",
    "    pairs = pairs.merge(u_ct.rename(columns={'UserId':'User_j','Country':'j_Country'}), on='User_j', how='left')\n",
    "    pairs['same_country'] = (pairs['i_Country'] == pairs['j_Country']).astype(int)\n",
    "else:\n",
    "    pairs['same_country'] = 0\n",
    "\n",
    "pairs['abs_rank_diff'] = (pairs['i_HighestRanking'] - pairs['j_HighestRanking']).abs().fillna(0)\n",
    "pairs['avg_rank'] = (pairs['i_HighestRanking'] + pairs['j_HighestRanking'])/2\n",
    "\n",
    "# avg_c_neg HHA-style\n",
    "pairs['avg_c_neg'] = -0.5 * (pairs['ci'] + pairs['cj'])\n",
    "\n",
    "# standardize HRank for i/j if present\n",
    "pairs['i_HRank_s'] = (pairs['i_HighestRanking'] - pairs['i_HighestRanking'].mean()) / max(1e-6, pairs['i_HighestRanking'].std())\n",
    "pairs['j_HRank_s'] = (pairs['j_HighestRanking'] - pairs['j_HighestRanking'].mean()) / max(1e-6, pairs['j_HighestRanking'].std())\n",
    "\n",
    "# ensure label numeric\n",
    "pairs['label'] = pairs['label'].astype(int)\n",
    "\n",
    "print(\"Pairs with covariates ready. Example:\")\n",
    "print(pairs[['CompetitionId','User_i','User_j','label','same_org','same_country','abs_rank_diff','avg_c_neg']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f154602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick sim test (init) mean P: 0.5\n",
      "Optimization finished in 3716.4s; success=False, msg=STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "Fixed coeffs (incl intercept): [ 7.58842674e-02  2.68745732e+00 -1.80502806e-03  4.93915469e-03\n",
      "  3.24828412e+00  7.88416565e-02 -3.83680216e-01  0.00000000e+00\n",
      "  0.00000000e+00 -3.04751280e-01 -2.81826564e-01]\n",
      "Random mean/sd: [0.] [0.2]\n",
      "Example predictions:\n",
      "   label  pred_prob\n",
      "0      1   0.212599\n",
      "1      1   0.038145\n",
      "2      1   0.490266\n",
      "3      1   0.127352\n",
      "4      1   0.383381\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Mixed-logit simulated MLE (random coef on same_org) - memory-friendly\n",
    "# -------------------------\n",
    "# Fixed and random covariates\n",
    "fixed_vars = ['same_country','abs_rank_diff','avg_rank','avg_c_neg','i_HRank_s','j_HRank_s','i_n_competitions','j_n_competitions','i_is_male','j_is_male']\n",
    "random_vars = ['same_org']\n",
    "\n",
    "df = pairs.reset_index(drop=True)\n",
    "y = df['label'].astype(float).values\n",
    "w = np.ones_like(y)   # uniform weights\n",
    "\n",
    "# build Xf, Xr\n",
    "Xf = df[fixed_vars].fillna(0).astype(float).values\n",
    "Xf = np.hstack([np.ones((Xf.shape[0],1)), Xf])  # intercept included\n",
    "Kf = Xf.shape[1]\n",
    "if len(random_vars) > 0:\n",
    "    Xr = df[random_vars].fillna(0).astype(float).values\n",
    "    Kr = Xr.shape[1]\n",
    "else:\n",
    "    Xr = np.zeros((Xf.shape[0],0)); Kr = 0\n",
    "N = Xf.shape[0]\n",
    "\n",
    "# Halton draws\n",
    "R_draws = 50\n",
    "if Kr > 0:\n",
    "    sampler = qmc.Halton(d=Kr, scramble=True, seed=0)\n",
    "    z = stats.norm.ppf(sampler.random(n=R_draws))\n",
    "else:\n",
    "    z = np.zeros((1,0))\n",
    "\n",
    "def pack(bf, mu, logsd):\n",
    "    return np.concatenate([bf, mu, logsd])\n",
    "\n",
    "def unpack(p):\n",
    "    bf = p[:Kf]\n",
    "    mu = p[Kf:Kf+Kr] if Kr>0 else np.array([])\n",
    "    logsd = p[Kf+Kr:Kf+2*Kr] if Kr>0 else np.array([])\n",
    "    return bf, mu, np.exp(logsd)\n",
    "\n",
    "batch_size = 20000\n",
    "def sim_probs_fast(p):\n",
    "    bf, mu, sd = unpack(p)\n",
    "    fixed_u = Xf.dot(bf)\n",
    "    if Kr == 0:\n",
    "        return np.clip(expit(fixed_u),1e-12,1-1e-12)\n",
    "    P = np.empty(N, dtype=np.float64)\n",
    "    R = z.shape[0]\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = min(N, start + batch_size)\n",
    "        fixed_b = fixed_u[start:end]\n",
    "        Xr_b = Xr[start:end,:]\n",
    "        Psum = np.zeros(end-start, dtype=np.float64)\n",
    "        for r in range(R):\n",
    "            beta_r = mu + sd * z[r,:]\n",
    "            Psum += expit(fixed_b + Xr_b.dot(beta_r))\n",
    "        P[start:end] = Psum / float(R)\n",
    "    return np.clip(P,1e-12,1-1e-12)\n",
    "\n",
    "def negll(p):\n",
    "    P = sim_probs_fast(p)\n",
    "    return -float(np.sum(w * (y * np.log(P) + (1-y) * np.log(1-P))))\n",
    "\n",
    "# init & optimize (light)\n",
    "bf0 = np.zeros(Kf)\n",
    "mu0 = np.zeros(Kr) if Kr>0 else np.array([])\n",
    "logsd0 = np.log(0.2 * np.ones(Kr)) if Kr>0 else np.array([])\n",
    "init = pack(bf0, mu0, logsd0)\n",
    "\n",
    "print(\"Quick sim test (init) mean P:\", float(sim_probs_fast(init).mean()))\n",
    "t0 = time.time()\n",
    "res = optimize.minimize(negll, init, method='L-BFGS-B', options={'maxiter':100, 'disp':True})\n",
    "t1 = time.time()\n",
    "print(\"Optimization finished in {:.1f}s; success={}, msg={}\".format(t1-t0, res.success, res.message))\n",
    "\n",
    "bf_hat, mu_hat, sd_hat = unpack(res.x)\n",
    "print(\"Fixed coeffs (incl intercept):\", bf_hat)\n",
    "if Kr>0:\n",
    "    print(\"Random mean/sd:\", mu_hat, sd_hat)\n",
    "\n",
    "# attach predicted probability\n",
    "df['pred_prob'] = sim_probs_fast(res.x)\n",
    "print(\"Example predictions:\")\n",
    "print(df[['label','pred_prob']].head())\n",
    "\n",
    "# Save results if desired\n",
    "# df.to_csv('pairs_with_preds.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
